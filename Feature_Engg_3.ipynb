{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Q1"
      ],
      "metadata": {
        "id": "LsCQZW1r351U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Min-Max Scaling is a data preprocessing technique used to normalize the range of features in a dataset. It transforms the values of a feature to a specific range, typically [0, 1] or [-1, 1], by applying the following formula:\n",
        "\n",
        "ð‘¥\n",
        "scaled\n",
        "=\n",
        "ð‘¥\n",
        "âˆ’\n",
        "ð‘¥\n",
        "min\n",
        "ð‘¥\n",
        "max\n",
        "âˆ’\n",
        "ð‘¥\n",
        "min\n",
        "x\n",
        "scaled\n",
        "â€‹\n",
        " =\n",
        "x\n",
        "max\n",
        "â€‹\n",
        " âˆ’x\n",
        "min\n",
        "â€‹\n",
        "\n",
        "x: Original value of the feature.\n",
        "\n",
        "x\n",
        "min\n",
        "â€‹\n",
        " : Minimum value of the feature.\n",
        "\n",
        "x\n",
        "max\n",
        "â€‹\n",
        " : Maximum value of the feature.\n",
        "\n",
        "x\n",
        "scaled\n",
        "â€‹\n",
        " : Scaled value of the feature.\n",
        "\n",
        "This scaling technique ensures that all features have the same scale, which is particularly important for algorithms sensitive to the magnitude of feature values, such as:\n",
        "\n",
        "Gradient-based models (e.g., logistic regression, neural networks).\n",
        "Distance-based algorithms (e.g., k-Nearest Neighbors, k-Means Clustering).\n",
        "\n",
        "Steps in Min-Max Scaling\n",
        "\n",
        "Compute the minimum (\n",
        "ð‘¥\n",
        "min\n",
        "x\n",
        "min\n",
        "â€‹\n",
        " ) and maximum (\n",
        "ð‘¥\n",
        "max\n",
        "x\n",
        "max\n",
        "â€‹\n",
        " ) values of the feature.\n",
        "\n",
        "Apply the scaling formula to each data point in the feature.\n",
        "Repeat the process for all features in the dataset.\n"
      ],
      "metadata": {
        "id": "K347bFuI354H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-QtRxHE3nOH",
        "outputId": "3a10b11d-5fa5-4df4-b25d-5fd1b5c014d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Age    Salary\n",
            "0  0.0  0.000000\n",
            "1  0.2  0.200000\n",
            "2  0.4  0.466667\n",
            "3  0.6  0.666667\n",
            "4  1.0  1.000000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Example Data\n",
        "data = {'Age': [25, 30, 35, 40, 50],\n",
        "        'Salary': [50000, 80000, 120000, 150000, 200000]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Min-Max Scaling\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Scaled Data\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=['Age', 'Salary'])\n",
        "print(scaled_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2"
      ],
      "metadata": {
        "id": "o8_Dssr5356p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Unit Vector technique, also called Normalization, scales each data point to have a unit norm (e.g., a magnitude of 1). It is achieved by dividing each data point by its norm (e.g., Euclidean norm or\n",
        "ð¿\n",
        "2\n",
        "L2-norm), which ensures that the magnitude of the data points is scaled to 1.\n",
        "\n",
        "For a feature vector\n",
        "ð‘¥\n",
        "=\n",
        "[\n",
        "ð‘¥\n",
        "1\n",
        ",\n",
        "ð‘¥\n",
        "2\n",
        ",\n",
        "â€¦\n",
        ",\n",
        "ð‘¥\n",
        "ð‘›\n",
        "]\n",
        "x=[x\n",
        "1\n",
        "â€‹\n",
        " ,x\n",
        "2\n",
        "â€‹\n",
        " ,â€¦,x\n",
        "n\n",
        "â€‹\n",
        " ], the normalized vector\n",
        "ð‘¥\n",
        "normalized\n",
        "x\n",
        "normalized\n",
        "â€‹\n",
        "  is calculated as:\n",
        "\n",
        "ð‘¥\n",
        "normalized\n",
        "=\n",
        "ð‘¥\n",
        "âˆ¥\n",
        "ð‘¥\n",
        "âˆ¥\n",
        "x\n",
        "normalized\n",
        "â€‹\n",
        " =\n",
        "âˆ¥xâˆ¥\n",
        "x\n",
        "â€‹\n",
        "\n",
        "Where\n",
        "âˆ¥\n",
        "ð‘¥\n",
        "âˆ¥\n",
        "âˆ¥xâˆ¥ is the norm of the vector, computed as:\n",
        "\n",
        "L2-Norm (Euclidean):\n",
        "\n",
        "âˆ¥\n",
        "ð‘¥\n",
        "âˆ¥\n",
        "2\n",
        "=\n",
        "ð‘¥\n",
        "1\n",
        "2\n",
        "+\n",
        "ð‘¥\n",
        "2\n",
        "2\n",
        "+\n",
        "â‹¯\n",
        "+\n",
        "ð‘¥\n",
        "ð‘›\n",
        "2\n",
        "âˆ¥xâˆ¥\n",
        "2\n",
        "â€‹\n",
        " =\n",
        "x\n",
        "1\n",
        "2\n",
        "â€‹\n",
        " +x\n",
        "2\n",
        "2\n",
        "â€‹\n",
        " +â‹¯+x\n",
        "n\n",
        "2\n",
        "â€‹\n",
        "\n",
        "â€‹\n",
        "\n",
        "L1-Norm (Manhattan):\n",
        "\n",
        "âˆ¥\n",
        "ð‘¥\n",
        "âˆ¥\n",
        "1\n",
        "=\n",
        "âˆ£\n",
        "ð‘¥\n",
        "1\n",
        "âˆ£\n",
        "+\n",
        "âˆ£\n",
        "ð‘¥\n",
        "2\n",
        "âˆ£\n",
        "+\n",
        "â‹¯\n",
        "+\n",
        "âˆ£\n",
        "ð‘¥\n",
        "ð‘›\n",
        "âˆ£\n",
        "âˆ¥xâˆ¥\n",
        "1\n",
        "â€‹\n",
        " =âˆ£x\n",
        "1\n",
        "â€‹\n",
        " âˆ£+âˆ£x\n",
        "2\n",
        "â€‹\n",
        " âˆ£+â‹¯+âˆ£x\n",
        "n\n",
        "â€‹\n",
        " âˆ£\n"
      ],
      "metadata": {
        "id": "1yWC8na0359b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "data = np.array([[3, 4], [1, 2], [5, 12], [2, 1], [0, 3]])\n",
        "\n",
        "# Normalize data (L2 norm by default)\n",
        "normalized_data = normalize(data, norm='l2')\n",
        "\n",
        "print(normalized_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9alMerh5Sn2",
        "outputId": "c5d44c73-43c2-4e60-9cb4-01e14e112e15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.6        0.8       ]\n",
            " [0.4472136  0.89442719]\n",
            " [0.38461538 0.92307692]\n",
            " [0.89442719 0.4472136 ]\n",
            " [0.         1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3"
      ],
      "metadata": {
        "id": "eAeN9LA735_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction while preserving as much variance as possible in the data. It achieves this by transforming the original features into a new set of uncorrelated variables called principal components, which are linear combinations of the original features. The principal components are ordered such that the first component captures the maximum variance, the second captures the next highest variance orthogonal to the first, and so on."
      ],
      "metadata": {
        "id": "MVHMyjiG36Cp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How PCA Works\n",
        "Standardize the Data: Ensure all features are on the same scale by standardizing them.\n",
        "\n",
        "Compute the Covariance Matrix: Calculate the covariance matrix of the standardized data to understand feature relationships.\n",
        "\n",
        "Find Eigenvalues and Eigenvectors: Determine the eigenvalues (variances) and eigenvectors (directions) of the covariance matrix.\n",
        "\n",
        "Select Principal Components: Choose the top\n",
        "ð‘˜\n",
        "k eigenvectors corresponding to the\n",
        "ð‘˜\n",
        "k largest eigenvalues.\n",
        "\n",
        "Transform the Data: Project the original data onto the selected eigenvectors to form the reduced-dimensional representation.\n",
        "#Applications of PCA\n",
        "Dimensionality Reduction: Simplifies datasets with many features while retaining the most critical information.\n",
        "\n",
        "Visualization: Reduces high-dimensional data to 2 or 3 dimensions for easy visualization.\n",
        "\n",
        "Noise Reduction: Removes less significant features or noise by excluding components with minimal variance.\n",
        "\n",
        "Preprocessing for Machine Learning: Reduces dimensionality to avoid overfitting and improve model performance."
      ],
      "metadata": {
        "id": "JM5TGpMu36Fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Example data\n",
        "data = np.array([\n",
        "    [2, 8, 4, 6],\n",
        "    [3, 6, 5, 8],\n",
        "    [4, 7, 6, 10],\n",
        "    [5, 5, 7, 12],\n",
        "    [6, 4, 8, 14]\n",
        "])\n",
        "\n",
        "# Step 1: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "data_standardized = scaler.fit_transform(data)\n",
        "\n",
        "# Step 2: Apply PCA\n",
        "pca = PCA(n_components=2)  # Reduce to 2 dimensions\n",
        "data_reduced = pca.fit_transform(data_standardized)\n",
        "\n",
        "# Print results\n",
        "print(\"Original Shape:\", data.shape)\n",
        "print(\"Reduced Shape:\", data_reduced.shape)\n",
        "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
        "print(\"Principal Components:\\n\", data_reduced)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GtbxoWE7zOw",
        "outputId": "df10745c-3252-4c68-aabd-cd88bdba3e43"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Shape: (5, 4)\n",
            "Reduced Shape: (5, 2)\n",
            "Explained Variance Ratio: [0.96300648 0.03699352]\n",
            "Principal Components:\n",
            " [[-2.82765352  0.0661481 ]\n",
            " [-1.07469155 -0.58739941]\n",
            " [-0.33913521  0.62047346]\n",
            " [ 1.41382676 -0.03307405]\n",
            " [ 2.82765352 -0.0661481 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4"
      ],
      "metadata": {
        "id": "sUN_Yi-n36II"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Relationship Between PCA and Feature Extraction\n",
        "Feature Extraction refers to the process of creating new features that are informative and represent the underlying patterns in the data. These new features are derived from the original dataset.\n",
        "\n",
        "Principal Component Analysis (PCA) is a feature extraction method that transforms the original features into a new set of uncorrelated features called principal components. These components are ordered by the amount of variance they capture in the data.\n",
        "\n",
        "The relationship lies in PCA's ability to extract the most relevant features (principal components) from a dataset, effectively summarizing the original data into fewer dimensions while retaining as much variance as possible.\n",
        "\n",
        "#How PCA is Used for Feature Extraction\n",
        "Identify Key Variance: PCA identifies the directions (principal components) that maximize variance in the dataset.\n",
        "\n",
        "Transform Data: PCA projects the data onto these principal components, creating new features that are combinations of the original features.\n",
        "\n",
        "Select Top Components: Only the top\n",
        "ð‘˜\n",
        "k components that capture most of the variance are retained as extracted features.\n",
        "\n",
        "This reduces the dimensionality of the data and removes redundancy while preserving the most critical information."
      ],
      "metadata": {
        "id": "QTCjONvf36LZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5"
      ],
      "metadata": {
        "id": "V-HmAjGL36O0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Min-Max Scaling for Preprocessing Data in a Recommendation System\n",
        "When building a recommendation system for a food delivery service, features like price, rating, and delivery time may have different ranges. Min-Max scaling is a preprocessing technique that rescales these features to a uniform range (e.g., 0 to 1), ensuring they contribute equally to the model.\n",
        "\n",
        "#Steps to Apply Min-Max Scaling\n",
        "Understand the Data: Identify the range of each feature. For example:\n",
        "\n",
        "Price: $5 to $50\n",
        "Rating: 1 to 5\n",
        "Delivery Time: 10 to 60 minutes\n",
        "Formula for Min-Max Scaling:\n",
        "\n",
        "ð‘‹\n",
        "scaled\n",
        "=\n",
        "ð‘‹\n",
        "âˆ’\n",
        "ð‘‹\n",
        "min\n",
        "ð‘‹\n",
        "max\n",
        "âˆ’\n",
        "ð‘‹\n",
        "min\n",
        "X\n",
        "scaled\n",
        "â€‹\n",
        " =\n",
        "X\n",
        "max\n",
        "â€‹\n",
        " âˆ’X\n",
        "min\n",
        "â€‹\n",
        "\n",
        "Xâˆ’X\n",
        "min\n",
        "â€‹\n",
        "\n",
        "â€‹\n",
        "\n",
        "This formula rescales each feature to a range of 0 to 1.\n",
        "\n",
        "Preprocess Features: Apply the Min-Max scaling to each feature separately.\n",
        "\n",
        "Use the Scaled Data: Feed the scaled data into the recommendation system model for consistent weighting.\n"
      ],
      "metadata": {
        "id": "xdV8xZsl8ihg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Dataset\n",
        "data = {\n",
        "    \"Food Item\": [\"Pizza\", \"Burger\", \"Sushi\", \"Sandwich\", \"Pasta\"],\n",
        "    \"Price\": [15, 8, 40, 10, 25],\n",
        "    \"Rating\": [4.5, 4.2, 4.8, 3.9, 4.7],\n",
        "    \"Delivery Time\": [30, 25, 50, 20, 40]\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 1: Initialize the Min-Max Scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Step 2: Scale the numerical features\n",
        "scaled_data = scaler.fit_transform(df[[\"Price\", \"Rating\", \"Delivery Time\"]])\n",
        "\n",
        "# Step 3: Create a DataFrame for scaled features\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=[\"Price\", \"Rating\", \"Delivery Time\"])\n",
        "\n",
        "# Combine scaled features with Food Item\n",
        "result = pd.concat([df[[\"Food Item\"]], scaled_df], axis=1)\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCq1nKB_8REl",
        "outputId": "d6324e9c-42bd-4604-9b7e-9dc14f2821f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Food Item    Price    Rating  Delivery Time\n",
            "0     Pizza  0.21875  0.666667       0.333333\n",
            "1    Burger  0.00000  0.333333       0.166667\n",
            "2     Sushi  1.00000  1.000000       1.000000\n",
            "3  Sandwich  0.06250  0.000000       0.000000\n",
            "4     Pasta  0.53125  0.888889       0.666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Why Use Min-Max Scaling?\n",
        "Uniform Weighting: Without scaling, features with larger ranges (e.g., delivery time) might dominate features with smaller ranges (e.g., ratings).\n",
        "\n",
        "Improved Model Performance: Many machine learning algorithms (e.g., KNN, SVM) are sensitive to feature magnitudes. Min-Max scaling helps these models perform better.\n",
        "\n",
        "Interpretability: Scaled data makes it easier to interpret feature contributions during analysis or visualization.\n",
        "\n",
        "#Application in Recommendation System\n",
        "Price: Helps recommend items within the customer's budget.\n",
        "\n",
        "Rating: Ensures highly rated items are prioritized.\n",
        "\n",
        "Delivery Time: Optimizes for faster delivery.\n",
        "\n",
        "By using Min-Max scaling, you ensure that all features contribute equally to\n",
        "\n",
        "the recommendation system, enhancing its accuracy and reliability.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GRoEA4KZ8ik6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q6"
      ],
      "metadata": {
        "id": "bc8-g_oa8k2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using PCA for Dimensionality Reduction in Stock Price Prediction\n",
        "When building a predictive model for stock prices, the dataset often contains numerous features such as company financial data (e.g., revenue, profit margins) and market trends (e.g., indices, sector performance). Principal Component Analysis (PCA) can help reduce the dataset's dimensionality, making it more manageable while retaining the most important information.\n",
        "\n",
        "#Steps to Apply PCA for Stock Price Prediction\n",
        "1. Understand the Data\n",
        "High Dimensionality: A dataset with many features may lead to computational inefficiency and overfitting.\n",
        "\n",
        "Correlated Features: Features like \"revenue\" and \"profit margin\" may be correlated, contributing redundant information.\n",
        "\n",
        "2. Preprocess the Data\n",
        "Standardize Features: PCA is sensitive to the scale of the data, so standardization is necessary to ensure all features contribute equally.\n",
        "ð‘§\n",
        "=\n",
        "ð‘¥\n",
        "âˆ’\n",
        "ðœ‡\n",
        "ðœŽ\n",
        "z=\n",
        "Ïƒ\n",
        "xâˆ’Î¼\n",
        "â€‹\n",
        "\n",
        "where\n",
        "ðœ‡\n",
        "Î¼ is the mean and\n",
        "ðœŽ\n",
        "Ïƒ is the standard deviation of the feature.\n",
        "\n",
        "3. Apply PCA\n",
        "Compute Principal Components: PCA identifies the directions (principal components) that capture the maximum variance in the data.\n",
        "\n",
        "Select the Number of Components: Use the explained variance ratio to decide how many principal components to retain (e.g., 95% variance).\n",
        "\n",
        "4. Use Reduced Data for Modeling\n",
        "Train the stock price prediction model using the reduced dataset with fewer components.\n"
      ],
      "metadata": {
        "id": "xcwK9ndQ8k9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Example: Financial and market data\n",
        "data = {\n",
        "    \"Revenue\": [500, 600, 700, 800, 900],\n",
        "    \"Profit Margin\": [0.15, 0.20, 0.18, 0.22, 0.25],\n",
        "    \"Market Index\": [3000, 3100, 3200, 3300, 3400],\n",
        "    \"Sector Growth\": [2.1, 2.3, 2.2, 2.4, 2.5]\n",
        "}\n",
        "\n",
        "# Step 1: Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 2: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Step 3: Apply PCA\n",
        "pca = PCA(n_components=2)  # Reduce to 2 components\n",
        "pca_data = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Step 4: Create a DataFrame for PCA results\n",
        "pca_df = pd.DataFrame(pca_data, columns=[\"PC1\", \"PC2\"])\n",
        "\n",
        "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
        "print(pca_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_F9-r8X8kj9",
        "outputId": "104a6743-8103-4650-b7ce-8d549c8e9900"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio: [0.95294948 0.04631811]\n",
            "        PC1       PC2\n",
            "0 -2.855419 -0.026639\n",
            "1 -0.707446  0.705701\n",
            "2 -0.646636 -0.651113\n",
            "3  1.354082 -0.054588\n",
            "4  2.855419  0.026639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q7"
      ],
      "metadata": {
        "id": "_Q75sHI18lKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Original dataset\n",
        "data = np.array([1, 5, 10, 15, 20])\n",
        "\n",
        "# Desired range for Min-Max scaling: [-1, 1]\n",
        "min_val, max_val = -1, 1\n",
        "\n",
        "# Min-Max scaling formula\n",
        "scaled_data = min_val + (data - data.min()) * (max_val - min_val) / (data.max() - data.min())\n",
        "\n",
        "print(\"Scaled Data:\", scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj1Ce3Ko90JR",
        "outputId": "40b8f504-a2de-48d0-f4b9-3af463d5d771"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled Data: [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q8"
      ],
      "metadata": {
        "id": "gEgW_PLo8lNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Performing Feature Extraction Using PCA\n",
        "For a dataset containing the features [height, weight, age, gender, blood pressure], the process of applying Principal Component Analysis (PCA) involves the following steps:\n",
        "\n",
        "#Steps for PCA-Based Feature Extraction\n",
        "1. Preprocessing\n",
        "Standardization: PCA is sensitive to feature scaling. Therefore, all features must be standardized to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "2. Apply PCA\n",
        "Compute the principal components to determine how much variance each component explains.\n",
        "\n",
        "3. Determine Number of Components\n",
        "Retain the principal components that collectively explain a significant percentage of the variance (e.g., 95%).\n",
        "\n",
        "#Choosing the Number of Principal Components\n",
        "Explanation\n",
        "\n",
        "Original Features: The dataset has 5 features. PCA will produce 5 principal components, each explaining a part of the variance.\n",
        "Explained Variance Ratio: Calculate the explained variance ratio to determine the contribution of each principal component.\n",
        "\n",
        "Threshold: Choose the number of components that explain a cumulative variance of 95% or more.\n",
        "\n"
      ],
      "metadata": {
        "id": "B2ifjSZN-D-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Example dataset (height, weight, age, gender, blood pressure)\n",
        "data = np.array([\n",
        "    [170, 65, 25, 1, 120],\n",
        "    [160, 70, 30, 0, 130],\n",
        "    [180, 80, 35, 1, 140],\n",
        "    [165, 55, 28, 0, 125],\n",
        "    [175, 75, 40, 1, 135]\n",
        "])\n",
        "\n",
        "# Step 1: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Step 2: Apply PCA\n",
        "pca = PCA()\n",
        "pca_data = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Step 3: Determine explained variance\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
        "print(\"Cumulative Variance:\", cumulative_variance)\n",
        "\n",
        "# Select components explaining at least 95% of the variance\n",
        "n_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
        "print(\"Number of Components to Retain:\", n_components)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWt1ijEY90Mv",
        "outputId": "2a87cb98-e3aa-4de7-bdd6-6fd25726fcb7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio: [0.71497306 0.19496105 0.05512943 0.03493645 0.        ]\n",
            "Cumulative Variance: [0.71497306 0.90993411 0.96506355 1.         1.        ]\n",
            "Number of Components to Retain: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e9fzKHgI-WJA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}